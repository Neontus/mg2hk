# -*- coding: utf-8 -*-
"""improveCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXAvFd98hLv1RCgm7Jy9BTnlwBxOA2DC

Improvements:
- Fixing + Cleaning Dataset
- Adding / Changing Layers
- Implementing k-fold cross validation
- Implementing GridSearch

### Inputs
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras

from google.colab import drive
drive.mount('/content/gdrive')

!unzip /content/gdrive/MyDrive/prepped.zip -d /content/

# import os
# to_remove = os.listdir('/content/dataset/fixing_dataset')
# aba = []
# for i in to_remove:
#   aba.append('/content/dataset/fixing_dataset/'+i)
# # print(*aba), !rm

"""### Loading Data + Normalizing Outliers"""

X = []
Y = []

!rm prepped/.DS_Store

import os
path = '/content/prepped/'
filenames = os.listdir(path)

for s in filenames:
  obs = np.load(path+'/'+s)
  data_x = obs['x']
  data_y = obs['y']
  X.append(np.transpose(data_x, [1, 2, 0]))
  Y.append(data_y)

"""### Data Augmentation"""

from sklearn.model_selection import train_test_split
test_size = 0.2
random_state = 0
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state = random_state)

x_train, x_test, y_train, y_test = np.asarray(x_train, dtype = object), np.asarray(x_test, dtype = object), np.asarray(y_train, dtype = object), np.asarray(y_test, dtype = object)

x_obsids = filenames.copy()
y_obsids = filenames.copy()
obs_x_train, obs_x_test, obs_y_train, obs_y_test = train_test_split(x_obsids, y_obsids, test_size = test_size, random_state = random_state)

print(len(x_train))
print(len(y_train))

"""### Custom Data Generator"""

from tensorflow.keras.utils import Sequence

class DataGenerator(Sequence):
    def __init__(self, X, Y, target_size, batch_size):
        self.X = X
        self.Y = Y
        self.target_size = target_size
        self.batch_size = batch_size

    def __len__(self):
        return int(len(self.X) / self.batch_size)

    def __getitem__(self, index):
        batch_X = self.X[index*self.batch_size : (index+1)*self.batch_size]
        batch_Y = self.Y[index*self.batch_size : (index+1)*self.batch_size]

        # Preprocess and generate data for the batch
        processed_X = np.asarray(self.preprocess_X(batch_X))
        processed_Y = np.asarray(self.preprocess_Y(batch_Y))

        #print(processed_X.shape)

        return processed_X, processed_Y

    def preprocess_X(self, batch_X):
        # Implement preprocessing logic for input data (X)
        # Return processed input data

        # Padding

        #print(batch_X[0].shape)
        padded = [self.padding_rgb(x) for x in batch_X]
        return padded

        pass

    def preprocess_Y(self, batch_Y):
        # Implement preprocessing logic for target data (Y)
        # Return processed target data
        padded = [self.padding_bw(y) for y in batch_Y]
        return padded

        pass

    def padding_rgb(self, image):
        #print("HERE", image.shape)

        ih, iw, c = image.shape
        fh, fw = self.target_size
        blank = (0,0,0)
        result = np.full((fh, fw, c), blank, dtype=np.float32)
        #print("checkpoint 1", result.dtype)

        x_center = (fw - iw) // 2
        y_center = (fh - ih) // 2

        result[y_center:y_center+ih,
              x_center:x_center+iw] = image

        return result
        pass

    def padding_bw(self, image):
        ih, iw = image.shape
        fh, fw = self.target_size
        blank = 0
        result = np.full((fh,fw), blank, dtype=np.float32)
        #print("checkpoint 2", result.dtype)

        x_center = (fw - iw) // 2
        y_center = (fh - ih) // 2

        result[y_center:y_center+ih,
              x_center:x_center+iw] = image

        return result
        pass

    def on_epoch_end(self):
        # Optional method to perform any action at the end of each epoch
        pass

max_x = np.nanmax([a.shape[1] for a in X])
max_y = np.nanmax([a.shape[0] for a in X])
target_size = (max_y, max_x)


batch_size = 16


train_gen = DataGenerator(X = x_train, Y = y_train, target_size = target_size, batch_size = batch_size)
test_gen = DataGenerator(X = x_test, Y = y_test, target_size = target_size, batch_size = batch_size)

"""### Custom Loss Function
- MSE for now
"""

import keras.backend as K

def loss(y_true, y_pred):

  mse = tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)

  return mse

"""### Building Model"""

from keras.models import Sequential
from keras.constraints import maxnorm
from keras.utils import np_utils
from keras import backend as K
from keras import regularizers, optimizers
from keras.layers import *
from keras import optimizers

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout, GaussianNoise, GaussianDropout
from keras.layers import Flatten, BatchNormalization
from keras.layers.convolutional import Conv2D, SeparableConv2D
from keras.constraints import maxnorm
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
from keras import backend as K
from keras import regularizers, optimizers

def create_model(dropout_rate = 0.3):
    model = Sequential()
    input_shape = [target_size[0], target_size[1], 3]
    model.add(Conv2D(16, (5, 5), input_shape = input_shape, activation = 'relu', padding = 'same'))
    model.add(Conv2D(16, (5, 5), activation = 'relu'))
    model.add(MaxPooling2D(pool_size = (2,2)))
    model.add(Dropout(dropout_rate))
    model.add(Conv2D(32, (5, 5), activation = 'relu'))
    model.add(Conv2D(32, (5, 5), activation = 'relu'))
    model.add(MaxPooling2D(pool_size = (2,2)))
    model.add(Dropout(dropout_rate))
    model.add(Conv2D(32, (5, 5), activation = 'relu'))
    model.add(Conv2D(32, (5, 5), activation = 'relu'))
    model.add(Conv2D(32, (5, 5), activation = 'relu'))
    model.add(Conv2DTranspose(32, (28, 28), output_padding = (1, 0), strides = (2, 2), activation = 'relu'))
    model.add(Conv2D(64, (5, 5), activation = 'relu'))
    model.add(Conv2D(64, (5, 5), activation = 'relu'))
    model.add(Conv2DTranspose(64, (35, 35), output_padding = (0, 1), strides = (2, 2), activation = 'relu'))
    model.add(Conv2D(64, (5, 5), activation = 'relu', padding = 'same'))
    model.add(Conv2D(64, (5, 5), activation = 'relu', padding = 'same'))
    model.add(Conv2D(1, (1, 1)))
    model.compile(loss = loss, optimizer = optimizers.Adam(learning_rate = 0.0001, amsgrad = True), metrics = [loss, 'accuracy'])

    return model

model = create_model()
model.summary()

print("input")
[print(i.shape, i.dtype) for i in model.inputs]
print("\noutput")
[print(o.shape, o.dtype) for o in model.outputs]
print("\nlayers")
[print(l.name, l.input_shape, l.output_shape,l.dtype) for l in model.layers]

from keras.callbacks import History
history = History()
model.fit(train_gen, steps_per_epoch = len(train_gen), validation_data = test_gen, validation_steps = len(test_gen), epochs = 30, use_multiprocessing = True, callbacks = [history])

"""### K-Fold Cross Validation"""

fold_var = 1

for train_index, val_index in kf.split(np.zeros(n),Y):
	training_data = train_data.iloc[train_index]
	validation_data = train_data.iloc[val_index]

  train_gen =
  test_gen =

	train_data_generator = idg.flow_from_dataframe(training_data, directory = image_dir,
						       x_col = "filename", y_col = "label",
						       class_mode = "categorical", shuffle = True)
	valid_data_generator  = idg.flow_from_dataframe(validation_data, directory = image_dir,
							x_col = "filename", y_col = "label",
							class_mode = "categorical", shuffle = True)

	# CREATE NEW MODEL
	model = create_model()
	# COMPILE NEW MODEL
	model.compile(loss='categorical_crossentropy',
		      optimizer=opt,
		      metrics=['accuracy'])

	# CREATE CALLBACKS
	checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var),
							monitor='val_accuracy', verbose=1,
							save_best_only=True, mode='max')
	callbacks_list = [checkpoint]
	# There can be other callbacks, but just showing one because it involves the model name
	# This saves the best model
	# FIT THE MODEL
	history = model.fit(train_data_generator,
			    epochs=num_epochs,
			    callbacks=callbacks_list,
			    validation_data=valid_data_generator)
	#PLOT HISTORY
	#		:
	#		:

	# LOAD BEST MODEL to evaluate the performance of the model
	model.load_weights("/saved_models/model_"+str(fold_var)+".h5")

	results = model.evaluate(valid_data_generator)
	results = dict(zip(model.metrics_names,results))

	VALIDATION_ACCURACY.append(results['accuracy'])
	VALIDATION_LOSS.append(results['loss'])

	tf.keras.backend.clear_session()

	fold_var += 1

"""### Data Visualization"""

predictions = model.predict(test_gen, steps = len(test_gen))

len(predictions)

inputs = []
corrects = []

for i in range(len(predictions)):
  input = []
  if i < 16:
    for j in range(3):
      input.append(test_gen.__getitem__(0)[0][i,:,:,j])

    inputs.append(input)
    corrects.append(test_gen.__getitem__(0)[1][i,:,:])
  else:
    print("HERE")
    for j in range(3):
      input.append(test_gen.__getitem__(1)[0][i-16,:,:,j])

    inputs.append(input)
    corrects.append(test_gen.__getitem__(1)[1][i-16,:,:])

np.nanmax(predictions[i])

i = 15

fig, ax = plt.subplots(2, 3, figsize = [30,24])
ax[0][0].imshow(inputs[i][0]) #1600
ax[0][1].imshow(inputs[i][1]) #1700
ax[0][2].imshow(inputs[i][2]) #304
ax[1][0].imshow(corrects[i], cmap='jet') #temperature
ax[1][1].imshow(predictions[i], cmap = 'jet') #predicted

#overlayed predicted and correct temp
ax[1][2].imshow(corrects[i], cmap = 'jet'); ax[1][2].imshow(predictions[i], alpha = 0.5*(corrects[i]>0), cmap = 'jet')

i = 15

data_cube = [inputs[i][0],inputs[i][1],inputs[i][2],corrects[i],predictions[i][:,:,0]]
to_save = np.stack(tuple(data_cube))
np.savez('/content/output/ahh.npz', data = to_save)

"""### 6/15 Meeting Prep"""

history.history.keys()

plt.plot(range(30), history.history['loss'], 'r')
plt.plot(range(30), history.history['val_loss'], 'b')
# plt.plot(range(30), history.history['accuracy'], 'g')
# plt.plot(range(30), history.history['val_accuracy'], 'purple')
# plt.legend(['Loss', 'Validation Loss', 'Accuracy', 'Validation Accuracy'])
plt.legend(['loss', 'val_loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')

